{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    if x < 0:\n",
    "        return 1 - 1 / (1 + math.exp(x))\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# Fungsi sigmoid untuk numpy array\n",
    "sigmoid_v = np.vectorize(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, layers, activation):\n",
    "        self.layers = layers\n",
    "        self.activation = activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, input_shape, denses):\n",
    "        self.denses = denses\n",
    "        self.input_shape = input_shape\n",
    "        self.weights = []\n",
    "\n",
    "    def set_weight(self):\n",
    "        last_dense = 0\n",
    "        for i, dense in enumerate(self.denses):\n",
    "            weight = []\n",
    "            if i == 0:\n",
    "                weight.append(np.random.randn(self.input_shape, dense.layers)*0.1)\n",
    "                # weight.append(np.random.randint(-100, 100, (self.input_shape, dense.layers)) * 0.01)\n",
    "            else:\n",
    "                weight.append(np.random.randn(last_dense, dense.layers)*0.1)\n",
    "                # weight.append(np.random.randint(-100, 100, (last_dense, dense.layers)) * 0.01)\n",
    "                \n",
    "            bias = []\n",
    "            bias.append(np.random.randn(1, dense.layers)*0.01)\n",
    "            # bias.append(np.random.randint(-100, 100, (1, dense.layers)) * 0.1)\n",
    "\n",
    "            last_dense = dense.layers\n",
    "\n",
    "            self.weights.append(weight)\n",
    "            self.weights.append(bias)\n",
    "\n",
    "    def get_weight(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def feedForward(self, X_train, y_train):\n",
    "        # print(\"feedforward\")\n",
    "        mat = []\n",
    "\n",
    "        it = iter(self.weights)\n",
    "        for i, w in enumerate(it):\n",
    "            bias = next(it)\n",
    "\n",
    "            if (isinstance(w, np.ndarray)):\n",
    "                w = w.tolist()\n",
    "            if (isinstance(bias, np.ndarray)):\n",
    "                bias = bias.tolist()\n",
    "\n",
    "            if i == 0:\n",
    "                result = np.matmul(X_train, w) + bias\n",
    "            else:\n",
    "                result = np.matmul(mat[-1], w) + bias\n",
    "\n",
    "            result = np.squeeze(result, axis=0)\n",
    "            mat.append(result)\n",
    "\n",
    "        # print(mat)\n",
    "        sig = []\n",
    "        for m in mat:\n",
    "            m = sigmoid_v(m)\n",
    "            sig.append(m)\n",
    "\n",
    "        mat = sig\n",
    "            \n",
    "        err = 0.5 * np.square(y_train - mat[-1])\n",
    " \n",
    "        return mat, err\n",
    "    \n",
    "    \n",
    "    def backPropagation(self, X_train, y_train, mat):\n",
    "        # print(\"backpropagation\")\n",
    "        delta = []\n",
    "        for i, m in enumerate(reversed(mat)):\n",
    "            if i == 0:\n",
    "                delta.append(m * (1 - m) * (y_train - m))\n",
    "            else:\n",
    "                if (isinstance(self.weights, np.ndarray)):\n",
    "                    self.weights = self.weights.tolist()\n",
    "\n",
    "                part_1 = m * (1 - m)\n",
    "                \n",
    "                part_2 = np.transpose(self.weights[len(self.weights) - (i * 2)])\n",
    "                \n",
    "                if (len(part_2.shape) == 3):\n",
    "                    part_2 = np.squeeze(part_2, axis=2)\n",
    "\n",
    "                part_3 = np.dot(delta[-1], part_2)\n",
    "\n",
    "                if (len(part_3.shape) == 3):\n",
    "                    part_3 = np.squeeze(part_3, axis=2)\n",
    "\n",
    "                result = part_1 * part_3\n",
    "\n",
    "                if (len(result.shape) == 3):\n",
    "                    result = np.squeeze(result, axis=0)\n",
    "                delta.append(result)\n",
    "\n",
    "        return delta\n",
    "    \n",
    "    def updateWeights(self, X_train, y_train, mat, delta, momentum, learning_rate, last_weight):\n",
    "        # print(\"updateWeights\")\n",
    "        new_weights = []\n",
    "\n",
    "        it = iter(reversed(self.weights))\n",
    "\n",
    "        for i, w in enumerate(it):\n",
    "            bias = w\n",
    "            weight = next(it)\n",
    "            \n",
    "            if (i == len(mat) - 1):\n",
    "                new_weight = learning_rate * (X_train.reshape(X_train.shape[0], 1) @ delta[i])\n",
    "                new_bias = learning_rate * delta[i]\n",
    "            else:\n",
    "                new_weight = learning_rate * (np.transpose(mat[len(mat) - i - 2]) @ delta[i])\n",
    "                new_bias = learning_rate * delta[i]\n",
    "            \n",
    "            if (len(new_weight.shape) == 3):\n",
    "                new_weight = np.squeeze(new_weight, axis=0)\n",
    "\n",
    "            if (momentum):\n",
    "                if last_weight:\n",
    "                    new_weight += momentum * np.squeeze(np.asarray(weight), axis=0)\n",
    "                    new_bias += momentum * np.squeeze(np.asarray(bias), axis=0)\n",
    "                else:\n",
    "                    new_weight += momentum * last_weight[len(last_weight) - (i * 2)]\n",
    "                    new_bias += momentum * last_weight[len(last_weight) - (i * 2) + 1]\n",
    "            \n",
    "            bias = np.asarray(bias)\n",
    "            bias = np.squeeze(bias, axis=0)\n",
    "            new_weights.insert(0, new_bias + bias)\n",
    "            new_weights.insert(0, new_weight + np.squeeze(np.asarray(weight), axis=0))\n",
    "\n",
    "        new_weights = np.asarray(new_weights)\n",
    "        new_weights = new_weights.reshape(new_weights.shape[0], 1)\n",
    "        \n",
    "        self.weights = new_weights\n",
    "    \n",
    "    def fit(self, X_train, y_train, epochs, batch_size, momentum, learning_rate):\n",
    "        self.set_weight()\n",
    "\n",
    "        list_err = []\n",
    "        last_weight = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle isi train data\n",
    "            from sklearn import utils\n",
    "            X_train, y_train = utils.shuffle(X_train, y_train)\n",
    "            \n",
    "            for i in range(len(X_train)):\n",
    "                mat, err = self.feedForward(X_train[i], y_train[i])\n",
    "                list_err.append(err[0][0])\n",
    "\n",
    "                if (i % batch_size == 0):\n",
    "                    delta = self.backPropagation(X_train[i], y_train[i], mat)\n",
    "                    last_weight = self.weights\n",
    "\n",
    "                    self.updateWeights(X_train[i], y_train, mat, delta, momentum, learning_rate, last_weight)\n",
    "        \n",
    "        return mat\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        prediction = []\n",
    "        list_err = []\n",
    "\n",
    "        for X in X_test:\n",
    "            mat = []\n",
    "\n",
    "            it = iter(self.weights)\n",
    "            for i, w in enumerate(it):\n",
    "                bias = next(it)\n",
    "                \n",
    "                if i == 0:\n",
    "                    X = X.astype(float)\n",
    "                    X = X.reshape(1, self.input_shape)\n",
    "\n",
    "                    result = np.dot(X, w[0]) + bias[0]\n",
    "                    mat.append(result)\n",
    "                else:\n",
    "                    result = np.dot(mat[-1], w[0]) + bias[0]\n",
    "                    mat.append(result)\n",
    "            \n",
    "            sig = []\n",
    "            for m in mat:\n",
    "                m = sigmoid_v(m)\n",
    "                sig.append(m)\n",
    "            mat = sig\n",
    "                \n",
    "            err = 0.5 * np.square(y_train - mat[-1])\n",
    "            list_err.append(err[0][0])\n",
    "\n",
    "            # print(mat[-1])\n",
    "            pred = np.greater(mat[-1], 0.5)\n",
    "            prediction.append(pred)\n",
    "            \n",
    "        prediction = np.asarray(prediction).astype(int)\n",
    "        prediction = prediction.squeeze(axis=1)\n",
    "        prediction = prediction.squeeze(axis=1)\n",
    "\n",
    "        return prediction, list_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pada make_blob dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X_train, y_train = make_blobs(n_samples=50, centers=2, n_features=2)\n",
    "\n",
    "# print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0\n",
      " 1 1 0 0 1 1 0 1 1 1 0 1 0]\n",
      "[0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0\n",
      " 1 1 0 0 1 1 0 1 1 1 0 1 0]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(2, [\n",
    "    Dense(3, 'sigmoid'),\n",
    "    Dense(2, 'sigmoid'),\n",
    "    Dense(1, 'sigmoid')\n",
    "])\n",
    "\n",
    "mat = model.fit(X_train, y_train, epochs=100, batch_size=5, momentum=0.001, learning_rate=0.02)\n",
    "prediction, list_err = model.predict(X_train)\n",
    "\n",
    "print(prediction)\n",
    "print(y_train)\n",
    "\n",
    "a = 0\n",
    "b = len(prediction)\n",
    "for i, j in zip(prediction, y_train):\n",
    "    if (i == j):\n",
    "        a += 1\n",
    "print(\"Accuracy: \", a/b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pada weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         0.88541667 0.        ]\n",
      " [1.         0.94117647 0.9375     1.        ]\n",
      " [0.         0.97647059 0.89583333 0.        ]\n",
      " [0.5        0.82352941 1.         0.        ]\n",
      " [0.5        0.8        0.83333333 0.        ]\n",
      " [0.5        0.76470588 0.72916667 1.        ]\n",
      " [0.         0.75294118 0.67708333 1.        ]\n",
      " [1.         0.84705882 0.98958333 0.        ]\n",
      " [1.         0.81176471 0.72916667 0.        ]\n",
      " [0.5        0.88235294 0.83333333 0.        ]\n",
      " [1.         0.88235294 0.72916667 1.        ]\n",
      " [0.         0.84705882 0.9375     1.        ]\n",
      " [0.         0.95294118 0.78125    0.        ]\n",
      " [0.5        0.83529412 0.94791667 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.io.arff import loadarff\n",
    "\n",
    "with open('weather.arff') as f:\n",
    "    data, meta = loadarff(f)\n",
    "    \n",
    "datas = []\n",
    "for i in data:\n",
    "    d = []\n",
    "    # print(i)\n",
    "    for j in i:\n",
    "        # print(j)\n",
    "        d.append(j)\n",
    "    datas.append(d)\n",
    "    \n",
    "datas = np.asarray(datas)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for i in data:\n",
    "    # data[i][0] = le.fit_transform(data[i][0])\n",
    "    datas[:, 0] = le.fit_transform(datas[:, 0])\n",
    "    datas[:, 3] = le.fit_transform(datas[:, 3])\n",
    "    datas[:, 4] = le.fit_transform(datas[:, 4])\n",
    "    \n",
    "X_train = datas[:, :-1]\n",
    "y_train = datas[:, -1:]\n",
    "\n",
    "X_train = X_train.astype(float)\n",
    "y_train = y_train.astype(int)\n",
    "y_train = y_train.squeeze(axis=1)\n",
    "# X_train = preprocessing.normalize(X_train)\n",
    "\n",
    "# print(X_train)\n",
    "x_normed = X_train / X_train.max(axis=0)\n",
    "# print(x_normed)\n",
    "X_train = x_normed\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 1 1 1 0 1 0 1 1 1 1 1 0]\n",
      "0.6428571428571429\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(4, [\n",
    "    Dense(3, 'sigmoid'),\n",
    "    Dense(2, 'sigmoid'),\n",
    "    Dense(1, 'sigmoid')\n",
    "])\n",
    "\n",
    "mat = model.fit(X_train, y_train, epochs=100, batch_size=1, momentum=0.001, learning_rate=0.02)\n",
    "prediction, list_err = model.predict(X_train)\n",
    "\n",
    "print(prediction)\n",
    "print(y_train)\n",
    "a = 0\n",
    "b = len(prediction)\n",
    "for i, j in zip(prediction, y_train):\n",
    "    if (i == j):\n",
    "        a += 1\n",
    "print(a/b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
